{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIUBWh3bLf6q",
        "outputId": "4011df63-7e74-4a43-ed4d-08b18bc15e53"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import regex as re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from tensorflow.keras.utils import pad_sequences, to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwcOBjbR9jaS"
      },
      "outputs": [],
      "source": [
        "def remove_unwanted_characters(txt) -> str:\n",
        "  newline_str = r\"\\n+\"\n",
        "  whitespace_str = r\"\\t|\\r\"\n",
        "  quote_str = r\"“|”|‘|’\"\n",
        "  roman_numeral_str = r\"(XI{0,2}\\.)|(VI{0,3}\\.)|(IV|IX|I{1,3}\\.)\"\n",
        "\n",
        "\n",
        "  txt = re.sub(newline_str, \" \", txt)\n",
        "  txt = re.sub(whitespace_str, \"\", txt)\n",
        "  txt = re.sub(quote_str, \"\", txt)\n",
        "  txt = re.sub(roman_numeral_str, \"\", txt)\n",
        "\n",
        "  return txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZuXAC-3-Jq8"
      },
      "outputs": [],
      "source": [
        "def file_to_sentences(FILE_PATH) -> list:\n",
        "  with open(FILE_PATH, \"r\") as file:\n",
        "    txt = remove_unwanted_characters(file.read())\n",
        "    # Split into sentences\n",
        "    sentences = sent_tokenize(txt)\n",
        "\n",
        "    return sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBwTCkLUF6DP"
      },
      "outputs": [],
      "source": [
        "FILE_PATH = \"../data/sherlock_holmes_text.txt\"\n",
        "\n",
        "sentences = file_to_sentences(FILE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMalLAgwPSs4"
      },
      "outputs": [],
      "source": [
        "sentences = sentences[4:] # Crops out the preface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7nobRrBPk6W",
        "outputId": "c69d4dd7-c4c2-47a5-c022-0acfb0a44740"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['In',\n",
              " 'his',\n",
              " 'eyes',\n",
              " 'she',\n",
              " 'eclipses',\n",
              " 'and',\n",
              " 'predominates',\n",
              " 'the',\n",
              " 'whole',\n",
              " 'of',\n",
              " 'her',\n",
              " 'sex',\n",
              " '.']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences = [word_tokenize(sent) for sent in sentences]\n",
        "sentences[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQLrm77MRYd8"
      },
      "outputs": [],
      "source": [
        "all_words = [word for sentence in sentences for word in sentence]\n",
        "vocabulary = set(all_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucQJh7dRRzf8",
        "outputId": "5e4553d7-b8aa-41e1-dc07-287cea239050"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(1, 'greater'),\n",
              " (2, 'borders'),\n",
              " (3, 'averse'),\n",
              " (4, 'obtaining'),\n",
              " (5, 'even'),\n",
              " (6, 'exit'),\n",
              " (7, 'Maggie'),\n",
              " (8, 'servants—a'),\n",
              " (9, 'personate'),\n",
              " (10, 'aunt')]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This is how enumerate works\n",
        "# We can use enumerate to create our word:idx mapping\n",
        "list(enumerate(vocabulary, 1))[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0TyRBotRi61"
      },
      "outputs": [],
      "source": [
        "# Using a list comprehension, we can loop through each (idx, word) pair from enumerate\n",
        "# Each pair holds the key and value we want in our word_to_idx dict\n",
        "# We start the enumeration from 1, not 0, because want 0 to represent the padding token\n",
        "word_to_idx = {word : idx for idx, word in enumerate(vocabulary, 1)}\n",
        "# Let's also create a idx_to_word dict so we can interpet the results of the model later\n",
        "idx_to_word = {idx : word for word, idx in word_to_idx.items()}\n",
        "vocab_size = len(vocabulary) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JpStpH6PyOj"
      },
      "outputs": [],
      "source": [
        "input_sequences = []\n",
        "for sentence in sentences:\n",
        "  # Convert the sentence to its numerical representation with the word_to_idx mapping\n",
        "  numerized_sentence = [word_to_idx[word] for word in sentence]\n",
        "  # Create ngrams from size 2 to the size of the sentence\n",
        "  for i in range(2, len(sentence) + 1):\n",
        "    ngram = numerized_sentence[:i]\n",
        "    input_sequences.append(ngram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lz_vA4YxQpvr",
        "outputId": "7c503788-11d6-4d08-8785-a709c620840c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[6289, 5601, 7877, 2611, 7934, 2018, 8284],\n",
              " [6289, 5601, 7877, 2611, 7934, 2018, 8284, 6925],\n",
              " [6289, 5601, 7877, 2611, 7934, 2018, 8284, 6925, 3097],\n",
              " [6289, 5601, 7877, 2611, 7934, 2018, 8284, 6925, 3097, 8863],\n",
              " [6289, 5601, 7877, 2611, 7934, 2018, 8284, 6925, 3097, 8863, 2940]]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_sequences[5:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60XQj1ZbSvkx"
      },
      "outputs": [],
      "source": [
        "# Now, let's pad the sequences so they are all the same length\n",
        "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
        "# pad_sequences adds 0s to the beginning of each array until size(vector) = max_sequence_len\n",
        "# This is why we started our enumeration from 1, not 0, because 0 represents the padding token\n",
        "# We use pre padding because padding at the end would cause us to lose the location of the label\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eD00FdvGQs2L"
      },
      "outputs": [],
      "source": [
        "X = [sequence[:-1] for sequence in input_sequences]\n",
        "y = [sequence[-1] for sequence in input_sequences]\n",
        "y = to_categorical(y, num_classes=vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqncApTRCHWk"
      },
      "source": [
        "## Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqu3ys-8CJp-"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization, SpatialDropout1D, GaussianNoise\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkJvwWUtCMqY"
      },
      "outputs": [],
      "source": [
        "# Building the RNN model\n",
        "model = Sequential()\n",
        "\n",
        "# Embedding layer\n",
        "model.add(Embedding(vocab_size, 400, input_length=max_sequence_len-1))  # Increased embedding dimensions\n",
        "model.add(SpatialDropout1D(0.25))\n",
        "model.add(GaussianNoise(0.1))\n",
        "\n",
        "# RNN 1\n",
        "model.add(LSTM(512, dropout=0.25, recurrent_dropout=0.25))  # Increased units, added dropout\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Final Layer\n",
        "model.add(Dense(vocab_size, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4_Ec4EEChor",
        "outputId": "14359757-102f-4f1c-d1ab-459edec53550"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 122, 400)          3785600   \n",
            "                                                                 \n",
            " spatial_dropout1d (Spatial  (None, 122, 400)          0         \n",
            " Dropout1D)                                                      \n",
            "                                                                 \n",
            " gaussian_noise (GaussianNo  (None, 122, 400)          0         \n",
            " ise)                                                            \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 512)               1869824   \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 512)               2048      \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 9464)              4855032   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10512504 (40.10 MB)\n",
            "Trainable params: 10511480 (40.10 MB)\n",
            "Non-trainable params: 1024 (4.00 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer=Adam(lr=0.01, clipnorm=1.0), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNRArC0-Cpyb"
      },
      "outputs": [],
      "source": [
        "# Stop the model early\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=1, restore_best_weights=True)\n",
        "# OPTIONAL: Reduce learning rate when the model stops improving, can help the gradient descent get out of local minima\n",
        "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.001, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRgOfaeICs5_"
      },
      "outputs": [],
      "source": [
        "# This will train the model; adjust epochs and batch size as necessary\n",
        "history = model.fit(X, y, epochs=200, batch_size=32, verbose=1, validation_split=0.2, callbacks=[early_stop])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gk5Oe9u4C7qf"
      },
      "outputs": [],
      "source": [
        "model.save('../exports/sherlock_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jk6UCE5PDGbc"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqzZzM1WC_qp"
      },
      "outputs": [],
      "source": [
        "model = load_model('../exports/sherlock_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYfME4S9DWIr",
        "outputId": "ca6e4130-bf78-49f4-fc02-87328a2b1502"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 122, 400)          3785600   \n",
            "                                                                 \n",
            " spatial_dropout1d (Spatial  (None, 122, 400)          0         \n",
            " Dropout1D)                                                      \n",
            "                                                                 \n",
            " gaussian_noise (GaussianNo  (None, 122, 400)          0         \n",
            " ise)                                                            \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 512)               1869824   \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 512)               2048      \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 9464)              4855032   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10512504 (40.10 MB)\n",
            "Trainable params: 10511480 (40.10 MB)\n",
            "Non-trainable params: 1024 (4.00 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Njlpg_NDESIX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def predict_next_word(model, text, max_sequence_len, word_to_index, index_to_word):\n",
        "    \"\"\"\n",
        "    Predict the next word based on the input text.\n",
        "\n",
        "    Args:\n",
        "    - model (tf.keras.Model): Trained model for prediction.\n",
        "    - text (str): Input string.\n",
        "    - max_sequence_len (int): Maximum length of input sequences.\n",
        "    - word_to_index (dict): Mapping from words to their respective indices.\n",
        "    - index_to_word (dict): Mapping from indices to their respective words.\n",
        "\n",
        "    Returns:\n",
        "    - str: Predicted word.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the input string\n",
        "    token_list = [word_to_index[word] for word in word_tokenize(text) if word in word_to_index]\n",
        "\n",
        "    # Pad the token sequence\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\n",
        "    # Predict the token of the next word\n",
        "    predicted_idx = np.argmax(model.predict(token_list), axis=-1)\n",
        "\n",
        "    # Convert the token back to a word\n",
        "    predicted_word = index_to_word.get(predicted_idx[0], '')\n",
        "\n",
        "    return predicted_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIK4mLuKFp6C"
      },
      "outputs": [],
      "source": [
        "def predict_next_n_words(model, text, n, max_sequence_len, word_to_index, index_to_word):\n",
        "    \"\"\"\n",
        "    Predict the next n words based on the input text.\n",
        "\n",
        "    Args:\n",
        "    - model (tf.keras.Model): Trained model for prediction.\n",
        "    - text (str): Input string.\n",
        "    - n (int): Number of words to predict.\n",
        "    - max_sequence_len (int): Maximum length of input sequences.\n",
        "    - word_to_index (dict): Mapping from words to their respective indices.\n",
        "    - index_to_word (dict): Mapping from indices to their respective words.\n",
        "\n",
        "    Returns:\n",
        "    - str: Predicted sequence of words.\n",
        "    \"\"\"\n",
        "\n",
        "    predicted_sequence = []\n",
        "\n",
        "    for _ in range(n):\n",
        "        # Tokenize the input string\n",
        "        token_list = [word_to_index[word] for word in word_tokenize(text) if word in word_to_index]\n",
        "\n",
        "        # Pad the token sequence\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\n",
        "        # Predict the token of the next word\n",
        "        predicted_idx = np.argmax(model.predict(token_list), axis=-1)\n",
        "\n",
        "        # Convert the token back to a word\n",
        "        predicted_word = index_to_word.get(predicted_idx[0], '')\n",
        "\n",
        "        # Append the predicted word to the sequence and to the text (for the next iteration)\n",
        "        predicted_sequence.append(predicted_word)\n",
        "        text += \" \" + predicted_word\n",
        "\n",
        "    return ' '.join(predicted_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjusz6qsEYUc",
        "outputId": "6dd5d340-1b49-48b9-faba-d5a4a1011fce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 533ms/step\n",
            "Sherlock said the biggest problem bequeathed\n"
          ]
        }
      ],
      "source": [
        "input_text = \"Sherlock said the biggest problem\"\n",
        "prediction = predict_next_word(model, input_text, max_sequence_len, word_to_idx, idx_to_word)\n",
        "print(input_text + \" \" + prediction)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
