{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory Next Word Prediction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seek to create a model that, given a string of text, can reliably predict the following *n* words. The model will be a Recurrent Neural Net w/ LSTM architecture.\n",
    "\n",
    "This particular notebook will use PyTorch rather than TensorFlow to take advantage of NVIDIA's CUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA supported by this system? True\n",
      "CUDA version: 12.1\n",
      "ID of current CUDA device: 0\n",
      "Name of current CUDA device: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "# Storing ID of current CUDA device\n",
    "cuda_id = torch.cuda.current_device()\n",
    "print(f\"ID of current CUDA device: {torch.cuda.current_device()}\")\n",
    "\t\n",
    "print(f\"Name of current CUDA device: {torch.cuda.get_device_name(cuda_id)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amira\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from util.process import Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"sherlock_holmes_text.txt\"\n",
    "\n",
    "sentences = Process.file_to_sentences(FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I have seldom heard him mention her under any other name.',\n",
       " 'In his eyes she eclipses and predominates the whole of her sex.',\n",
       " 'It was not that he felt any emotion akin to love for Irene Adler.',\n",
       " 'All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind.',\n",
       " 'He was, I take it, the most perfect reasoning and observing machine that the world has seen, but as a lover he would have placed himself in a false position.',\n",
       " 'He never spoke of the softer passions, save with a gibe and a sneer.',\n",
       " 'They were admirable things for the observerâ€”excellent for drawing the veil from mens motives and actions.',\n",
       " 'But for the trained reasoner to admit such intrusions into his own delicate and finely adjusted temperament was to introduce a distracting factor which might throw a doubt upon all his mental results.',\n",
       " 'Grit in a sensitive instrument, or a crack in one of his own high-power lenses, would not be more disturbing than a strong emotion in a nature such as his.',\n",
       " 'And yet there was but one woman to him, and that woman was the late Irene Adler, of dubious and questionable memory.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sentences[4:]\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'have',\n",
       " 'seldom',\n",
       " 'heard',\n",
       " 'him',\n",
       " 'mention',\n",
       " 'her',\n",
       " 'under',\n",
       " 'any',\n",
       " 'other',\n",
       " 'name',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'have',\n",
       " 'seldom',\n",
       " 'heard',\n",
       " 'him',\n",
       " 'mention',\n",
       " 'her',\n",
       " 'under',\n",
       " 'any',\n",
       " 'other']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = [word for sentence in sentences for word in sentence]\n",
    "all_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set(all_words)\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocabulary, 1)}\n",
    "index_to_word = {idx: word for word, idx in word_to_index.items()   }\n",
    "# The size of the vocabulary will be one larger because \n",
    "# we reserve integer 0 for the padding token\n",
    "vocab_size = len(vocabulary) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for sentence in sentences:\n",
    "    token_list = [word_to_index[word] for word in sentence]\n",
    "    for i in range(2, len(token_list) + 1):\n",
    "        ngram = token_list[:i]\n",
    "        input_sequences.append(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
    "\n",
    "X, y = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 500)\n",
    "        self.fc2 = nn.Linear(500, vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        # self.batch_norm = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        # print(f\"After embedding: {x.shape}\")\n",
    "        x, _ = self.lstm1(x)\n",
    "        # print(f\"After first LSTM: {x.shape}\")\n",
    "        # x = self.batch_norm(x)\n",
    "        # print(f\"After batch norm: {x.shape}\")\n",
    "        x, _ = self.lstm2(x)\n",
    "        # print(f\"After second LSTM: {x.shape}\")\n",
    "        x = self.dropout(x)\n",
    "        x = x[:, -1, :] # Sequence to label\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 128\n",
    "hidden_dim = 164\n",
    "\n",
    "# Initialize the model\n",
    "model = MyModel(vocab_size, embedding_dim, hidden_dim, vocab_size).to(device)\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X and y to PyTorch tensors\n",
    "X_tensor = torch.tensor(X)\n",
    "y_tensor = torch.tensor(y)\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "dataset = MyDataset(X_tensor, y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = 0.2\n",
    "batch_size = 64\n",
    "\n",
    "train_size = int((1 - validation_split) * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,    0,    0,  ..., 4451, 7074,  352],\n",
      "        [   0,    0,    0,  ...,    0,    0, 2617],\n",
      "        [   0,    0,    0,  ..., 4786, 4648, 1122],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ..., 2157, 8237, 2103],\n",
      "        [   0,    0,    0,  ..., 4098, 5701, 8237],\n",
      "        [   0,    0,    0,  ..., 4604, 1197, 8021]], dtype=torch.int32) tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[   0,    0,    0,  ..., 6560, 5291, 8237],\n",
      "        [   0,    0,    0,  ...,    0, 2969, 1312],\n",
      "        [   0,    0,    0,  ..., 8823, 6369, 4615],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ..., 7744, 7984, 6725],\n",
      "        [   0,    0,    0,  ..., 3488,  144, 7665],\n",
      "        [   0,    0,    0,  ..., 5121, 8237, 3790]], dtype=torch.int32) tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "for i, (inputs, labels) in enumerate(train_loader):\n",
    "    print(inputs, labels)\n",
    "    if i == 1:  # print the first 2 batches\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epoch, epochs=100):\n",
    "    start_time = time.time()\n",
    "    # Training\n",
    "    model.train() \n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for input_batch, target_batch in train_loader:  # Loop over batches of data\n",
    "        input_batch, target_batch = input_batch.to(device), target_batch.to(device) \n",
    "        optimizer.zero_grad()  \n",
    "        output = model(input_batch)\n",
    "        loss = criterion(output, target_batch)  \n",
    "        loss.backward()    \n",
    "        optimizer.step()\n",
    "\n",
    "    try:\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = 100 * correct_train / total_train  \n",
    "    except ZeroDivisionError:\n",
    "        print(\"ZeroDivisionError\")\n",
    "        train_loss = 0\n",
    "        train_accuracy = 0\n",
    "\n",
    "   # Validation\n",
    "    model.eval() \n",
    "\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_batch_val, target_batch_val in val_loader:\n",
    "            input_batch_val, target_batch_val = input_batch_val.to(device), target_batch_val.to(device)\n",
    "            \n",
    "            # Ensure target is not one-hot and is long type\n",
    "            if len(target_batch_val.shape) > 1:\n",
    "                target_batch_val = torch.argmax(target_batch_val, dim=1)\n",
    "            target_batch_val = target_batch_val.long()\n",
    "            \n",
    "            val_output = model(input_batch_val)\n",
    "            \n",
    "            # Log Shapes for Debugging\n",
    "            print(\"Output shape: \", val_output.shape)\n",
    "            print(\"Target shape: \", target_batch_val.shape)\n",
    "            \n",
    "            val_loss = criterion(val_output, target_batch_val)\n",
    "            running_val_loss += val_loss.item()\n",
    "            \n",
    "            _, predicted_val = torch.max(val_output, dim=1)\n",
    "            total_val += target_batch_val.size(0)\n",
    "            correct_val += (predicted_val == target_batch_val).sum().item()\n",
    "\n",
    "\n",
    "    try:\n",
    "        val_loss = running_val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * correct_val / total_val\n",
    "    except ZeroDivisionError:\n",
    "        print(\"ZeroDivisionError\")\n",
    "        val_loss = 0\n",
    "        val_accuracy = 0\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    # Logging\n",
    "    print(f'Epoch {epoch+1}/{epochs}, '\n",
    "          f'Time: {epoch_time:.2f}s, '\n",
    "          f'Loss: {train_loss:.4f}, '\n",
    "          f'Accuracy: {train_accuracy:.2f}%, '\n",
    "          f'Val Loss: {val_loss:.4f}, '\n",
    "          f'Val Accuracy: {val_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dry run to make sure everything is fine\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Training on GPU...\")\n",
    "else:\n",
    "    print(\"Training on CPU...\")\n",
    "\n",
    "train_model(0, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_model(epoch, epochs)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model\n",
    "torch.save(model.state_dict(), 'cudnn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating model\n",
    "model = MyModel(vocab_size, embedding_dim, hidden_dim, vocab_size).to(device)\n",
    "\n",
    "# Loading model\n",
    "model.load_state_dict(torch.load('path_to_save_model/model_name.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
