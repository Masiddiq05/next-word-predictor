{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory Next Word Prediction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seek to create a model that, given a string of text, can reliably predict the following *n* words. The model will be a Recurrent Neural Net w/ LSTM architecture.\n",
    "\n",
    "This particular notebook will use PyTorch rather than TensorFlow to take advantage of NVIDIA's CUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA supported by this system? True\n",
      "CUDA version: 12.1\n",
      "ID of current CUDA device: 0\n",
      "Name of current CUDA device: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "# Storing ID of current CUDA device\n",
    "cuda_id = torch.cuda.current_device()\n",
    "print(f\"ID of current CUDA device: {torch.cuda.current_device()}\")\n",
    "\t\n",
    "print(f\"Name of current CUDA device: {torch.cuda.get_device_name(cuda_id)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amira\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from util.process import Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"sherlock_holmes_text.txt\"\n",
    "\n",
    "sentences = Process.file_to_sentences(FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I have seldom heard him mention her under any other name.',\n",
       " 'In his eyes she eclipses and predominates the whole of her sex.',\n",
       " 'It was not that he felt any emotion akin to love for Irene Adler.',\n",
       " 'All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind.',\n",
       " 'He was, I take it, the most perfect reasoning and observing machine that the world has seen, but as a lover he would have placed himself in a false position.',\n",
       " 'He never spoke of the softer passions, save with a gibe and a sneer.',\n",
       " 'They were admirable things for the observerâ€”excellent for drawing the veil from mens motives and actions.',\n",
       " 'But for the trained reasoner to admit such intrusions into his own delicate and finely adjusted temperament was to introduce a distracting factor which might throw a doubt upon all his mental results.',\n",
       " 'Grit in a sensitive instrument, or a crack in one of his own high-power lenses, would not be more disturbing than a strong emotion in a nature such as his.',\n",
       " 'And yet there was but one woman to him, and that woman was the late Irene Adler, of dubious and questionable memory.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sentences[4:]\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'have',\n",
       " 'seldom',\n",
       " 'heard',\n",
       " 'him',\n",
       " 'mention',\n",
       " 'her',\n",
       " 'under',\n",
       " 'any',\n",
       " 'other',\n",
       " 'name',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'have',\n",
       " 'seldom',\n",
       " 'heard',\n",
       " 'him',\n",
       " 'mention',\n",
       " 'her',\n",
       " 'under',\n",
       " 'any',\n",
       " 'other']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = [word for sentence in sentences for word in sentence]\n",
    "all_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set(all_words)\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocabulary, 1)}\n",
    "index_to_word = {idx: word for word, idx in word_to_index.items()   }\n",
    "# The size of the vocabulary will be one larger because \n",
    "# we reserve integer 0 for the padding token\n",
    "vocab_size = len(vocabulary) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for sentence in sentences:\n",
    "    token_list = [word_to_index[word] for word in sentence]\n",
    "    for i in range(2, len(token_list) + 1):\n",
    "        ngram = token_list[:i]\n",
    "        input_sequences.append(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
    "\n",
    "X, y = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 500)\n",
    "        self.fc2 = nn.Linear(500, vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        # self.batch_norm = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        # print(f\"After embedding: {x.shape}\")\n",
    "        x, _ = self.lstm1(x)\n",
    "        # print(f\"After first LSTM: {x.shape}\")\n",
    "        # x = self.batch_norm(x)\n",
    "        # print(f\"After batch norm: {x.shape}\")\n",
    "        x, _ = self.lstm2(x)\n",
    "        # print(f\"After second LSTM: {x.shape}\")\n",
    "        x = self.dropout(x)\n",
    "        x = x[:, -1, :] # Sequence to label\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 128\n",
    "hidden_dim = 164\n",
    "\n",
    "# Initialize the model\n",
    "model = MyModel(vocab_size, embedding_dim, hidden_dim, vocab_size).to(device)\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X and y to PyTorch tensors\n",
    "X_tensor = torch.tensor(X)\n",
    "y_tensor = torch.tensor(y)\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "dataset = MyDataset(X_tensor, y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = 0.2\n",
    "batch_size = 64\n",
    "\n",
    "train_size = int((1 - validation_split) * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,    0,    0,  ..., 5058, 6766, 8394],\n",
      "        [   0,    0,    0,  ...,    0,  612, 1798],\n",
      "        [   0,    0,    0,  ..., 3711, 4536, 8412],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ..., 8412, 8394, 1255],\n",
      "        [   0,    0,    0,  ..., 2828, 1798, 7313],\n",
      "        [   0,    0,    0,  ..., 6095, 5904, 3886]], dtype=torch.int32) tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[   0,    0,    0,  ..., 9326, 2914, 5880],\n",
      "        [   0,    0,    0,  ..., 8300, 8412, 5390],\n",
      "        [   0,    0,    0,  ...,    0, 1892, 6722],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ..., 5446, 1461, 6732],\n",
      "        [   0,    0,    0,  ...,    0,    0,  486],\n",
      "        [   0,    0,    0,  ..., 3162, 8394, 2579]], dtype=torch.int32) tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "for i, (inputs, labels) in enumerate(train_loader):\n",
    "    print(inputs, labels)\n",
    "    if i == 1:  # print the first 2 batches\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epoch, epochs=100):\n",
    "    start_time = time.time()\n",
    "    # Training\n",
    "    model.train() \n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for input_batch, target_batch in train_loader:  # Loop over batches of data\n",
    "        input_batch, target_batch = input_batch.to(device), target_batch.to(device) \n",
    "        optimizer.zero_grad()  \n",
    "        output = model(input_batch)\n",
    "        loss = criterion(output, target_batch)  \n",
    "        loss.backward()    \n",
    "        optimizer.step()\n",
    "\n",
    "    try:\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = 100 * correct_train / total_train  \n",
    "    except ZeroDivisionError:\n",
    "        print(\"ZeroDivisionError\")\n",
    "        train_loss = 0\n",
    "        train_accuracy = 0\n",
    "\n",
    "    # Validation\n",
    "    model.eval() \n",
    "\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_batch_val, target_batch_val in val_loader:\n",
    "            input_batch_val, target_batch_val = input_batch_val.to(device), target_batch_val.to(device)\n",
    "            val_output = model(input_batch_val)\n",
    "            val_loss = criterion(val_output, target_batch_val)\n",
    "            \n",
    "            running_val_loss += val_loss.item()\n",
    "            _, predicted_val = torch.max(val_output.data, 1)\n",
    "            total_val += target_batch_val.size(0)\n",
    "            correct_val += (predicted_val == target_batch_val).sum().item()\n",
    "\n",
    "    try:\n",
    "        val_loss = running_val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * correct_val / total_val\n",
    "    except ZeroDivisionError:\n",
    "        print(\"ZeroDivisionError\")\n",
    "        val_loss = 0\n",
    "        val_accuracy = 0\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    # Logging\n",
    "    print(f'Epoch {epoch+1}/{epochs}, '\n",
    "          f'Time: {epoch_time:.2f}s, '\n",
    "          f'Loss: {train_loss:.4f}, '\n",
    "          f'Accuracy: {train_accuracy:.2f}%, '\n",
    "          f'Val Loss: {val_loss:.4f}, '\n",
    "          f'Val Accuracy: {val_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU...\n",
      "ZeroDivisionError\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (9464) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\amira\\OneDrive\\Documents\\GitHub\\Projects\\next-word-predictor\\models\\cudnn_rnn.ipynb Cell 29\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amira/OneDrive/Documents/GitHub/Projects/next-word-predictor/models/cudnn_rnn.ipynb#X43sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amira/OneDrive/Documents/GitHub/Projects/next-word-predictor/models/cudnn_rnn.ipynb#X43sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining on CPU...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/amira/OneDrive/Documents/GitHub/Projects/next-word-predictor/models/cudnn_rnn.ipynb#X43sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m train_model(\u001b[39m0\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\amira\\OneDrive\\Documents\\GitHub\\Projects\\next-word-predictor\\models\\cudnn_rnn.ipynb Cell 29\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amira/OneDrive/Documents/GitHub/Projects/next-word-predictor/models/cudnn_rnn.ipynb#X43sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m         _, predicted_val \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(val_output\u001b[39m.\u001b[39mdata, \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amira/OneDrive/Documents/GitHub/Projects/next-word-predictor/models/cudnn_rnn.ipynb#X43sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m         total_val \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m target_batch_val\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/amira/OneDrive/Documents/GitHub/Projects/next-word-predictor/models/cudnn_rnn.ipynb#X43sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m         correct_val \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (predicted_val \u001b[39m==\u001b[39;49m target_batch_val)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amira/OneDrive/Documents/GitHub/Projects/next-word-predictor/models/cudnn_rnn.ipynb#X43sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amira/OneDrive/Documents/GitHub/Projects/next-word-predictor/models/cudnn_rnn.ipynb#X43sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     val_loss \u001b[39m=\u001b[39m running_val_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(val_loader)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (9464) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Dry run to make sure everything is fine\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Training on GPU...\")\n",
    "else:\n",
    "    print(\"Training on CPU...\")\n",
    "\n",
    "train_model(0, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZeroDivisionError\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (9464) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\amira\\OneDrive\\Documents\\GitHub\\Projects\\next-word-predictor\\models\\cudnn_rnn.ipynb Cell 30\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amira/OneDrive/Documents/GitHub/Projects/next-word-predictor/models/cudnn_rnn.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amira/OneDrive/Documents/GitHub/Projects/next-word-predictor/models/cudnn_rnn.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/amira/OneDrive/Documents/GitHub/Projects/next-word-predictor/models/cudnn_rnn.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     train_model(epoch, epochs)    \n",
      "\u001b[1;32mc:\\Users\\amira\\OneDrive\\Documents\\GitHub\\Projects\\next-word-predictor\\models\\cudnn_rnn.ipynb Cell 30\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amira/OneDrive/Documents/GitHub/Projects/next-word-predictor/models/cudnn_rnn.ipynb#X31sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m         _, predicted_val \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(val_output\u001b[39m.\u001b[39mdata, \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amira/OneDrive/Documents/GitHub/Projects/next-word-predictor/models/cudnn_rnn.ipynb#X31sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m         total_val \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m target_batch_val\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/amira/OneDrive/Documents/GitHub/Projects/next-word-predictor/models/cudnn_rnn.ipynb#X31sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m         correct_val \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (predicted_val \u001b[39m==\u001b[39;49m target_batch_val)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amira/OneDrive/Documents/GitHub/Projects/next-word-predictor/models/cudnn_rnn.ipynb#X31sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amira/OneDrive/Documents/GitHub/Projects/next-word-predictor/models/cudnn_rnn.ipynb#X31sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     val_loss \u001b[39m=\u001b[39m running_val_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(val_loader)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (9464) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_model(epoch, epochs)    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
